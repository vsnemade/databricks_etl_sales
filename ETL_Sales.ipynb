# Set your access key config
spark.conf.set(
    "fs.azure.account.key.stgdatalakedemo.dfs.core.windows.net",
    "cHDAU95MEKkZvkiAJJw4Ncm65tdk9MtTF+THak6ujkf70uIila1oqWB3B6M+sYA4ZtKr/kXmwXDF+AStF/XFCA=="
)

# Read csv data from ADLS Gen2
df = spark.read.option("header", True).csv("abfss://rawdata@stgdatalakedemo.dfs.core.windows.net/sales_data.csv")
df.show()

# Sample Transformation: Filter only Electronics
electronics_df = df.filter(df.Category == "Electronics")
electronics_df.show()

# Save Filtered Data in Parquet Format to ADLS
electronics_df.write.mode("overwrite").parquet("abfss://rawdata@stgdatalakedemo.dfs.core.windows.net/processeddata/electronics")

# Read Parquet Again and Query
df_parquet = spark.read.parquet("abfss://rawdata@stgdatalakedemo.dfs.core.windows.net/processeddata/electronics")
df_parquet.createOrReplaceTempView("electronics_sales")

# SQL Query
result = spark.sql("SELECT Customer, Amount FROM electronics_sales WHERE Amount > 1500")
result.show()
